# 퍼셉트론 

퍼셉트론을 시작으로 여러 분류 모델에 대한 공부 및 리뷰를 남기고자 합니다. 리뷰 내용은 길벗에서 출판한 책을 참고하였습니다.   
퍼셉트론은 뇌 세포인 뉴런의 개념을 컴퓨터 학습에 적용한 머신러닝의 초기 모델로 두 개의 클래스가 있는 이진 분류를 해결하는 모델입니다.
그중에서도 오늘 살펴볼 퍼셉트론은 로젠블라트의 퍼셉트론 알고리즘입니다. 
두 개의 클래스는 1, -1로 나타낼 수 있으며, input x와 가중치 벡터 w의 선형 조합인 결정함수 $\phi{(z)}$ 를 통해 클래스를 결정 합니다.
선형 조합 z 의 형태는 다음과 같습니다. 

$$
z = w_1x_1 + w_2x_2 + \cdots +w_mx_m
$$

$$
w=\begin{bmatrix}w_1\\\vdots \\w_{m}\end{bmatrix}, \:  x=\begin{bmatrix}x_{1}\\\vdots\\x_{m}\end{bmatrix}
$$


input $x^{(i)}$ 가 결정함수 $\phi{(z)}$ 를 통해 출력되었을 때, 임계값 $\theta$ 보다 크면 클래스 1로 예측하고 그렇지 않으면 클래스 -1로 예측합니다. step function의 형태와 동일합니다. 아래의 결정 함수에서 임계값 $\theta$ 는 0으로 설정했습니다.

$$
\phi(z)=\begin{cases}
\;\;1& ,\:  z\ge 0\\
-1& ,\:  그 외
\end{cases}
$$
초기 퍼셉트론 크게  2스텝으로 구성된 간단한 구조입니다.
1. 가중치를 0 또는 랜덤한 작은 값으로 초기화
2. 각 훈련 샘플 $x^{(i)}$ 에서 출력 값 $\hat{y}$ 을 계산하고, 가중치 업데이트 (출력 값은 클래스예측 값)

$$
w_j := w_j + {\Delta}w_j
$$

$$
{\Delta}w_j = {\eta}(y^{(i)}-\hat{y}^{(i)})x_j^{(i)})
$$

$\eta$는 학습률(learning rate)이며 $y^{(i)}$ 는 실제 레이블,  $\hat{y}^{(i)}$ 는 예측 레이블입니다. 식에서 알 수 있듯이 퍼셉트론이 클래스 레이블을 정확히 예측한 경우 $y^{(i)} = \hat{y}^{(i)}$ 가중치는 업데이트 되지 않고 그대로 유지됩니다. 따라서 업데이트 값은 0 이 됩니다. 출력을 하거나, 하지않거나 2가지 경우만 있는 것입니다. 또한 가중치 벡터의 모든 가중치를 동시에 업데이트한다는 점이 중요합니다. 모든 가중치가 각 업데이트 $\Delta{w_j}$ 에 의해 업데이트되기전에 예측 레이블 $\hat{y}^{(i)}$ 를 다시 계산하지 않습니다.  


# 퍼셉트론 구현

이제 파이썬에서 퍼셉트론을 이용한 단순 분류 모델을 구현하고 iris 데이터에 적용해보겠습니다.


```python
class Perceptron(object):
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y):
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.errors_ = []

        for _ in range(self.n_iter):
            errors = 0
            for xi, target in zip(X, y):
                update = self.eta * (target - self.predict(xi))
                self.w_[1:] += update * xi
                self.w_[0] += update
                errors += int(update != 0.0)
            self.errors_.append(errors)
        return self

    def net_input(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def predict(self, X):
        return np.where(self.net_input(X) >= 0.0, 1, -1)
```

- 학습률 eta와 에포크 횟수 n_iter로 Perceptron 객체를 초기화 하도록 설정했습니다.  
- fit 에서 self.w_ 가중치를 벡터 $\mathbb{R}^{m+1}$ 로 초기화합니다. 여기서 $m$은 데이터셋의 차원 수 입니다.   

- 이후 numpy에서 제공하는 난수 생성기를 사용하여 가중치를 초기화 했습니다.   
- rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])은 표준편차가 0.01인 정규분포에서 랜덤 샘플링한 아주 작은 수 입니다. 가중치를 0으로 초기화 하지 않는 것은 가중치가 0이 아니어야 $\eta$가 분류결과에 영향을 주기 때문입니다. 0으로 초기화 된다면 $\eta$ 는 가중치 벡터의 방향이 아닌 크기에만 영향을 미치게 됩니다. 


```python
# iris
import pandas as pd
from sklearn.datasets import load_iris

iris = load_iris()
df = pd.DataFrame(data = iris.data, columns = (iris.feature_names)) 
df['target'] = iris.target
df['target'] = df['target'].map({0:'setosa', 1:"versicolor", 2:'virginica'})
```


```python
df.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
  </tbody>
</table>
</div>![output_10_1](C:\Users\94tjd\Downloads\Untitled\output_10_1.png



- 초기 퍼셉트론이 이진 분류기이기 때문에 iris 데이터셋에서 두 개의 꽃 setosa와 versicolor만 사용하겠습니다. 이후 포스팅에서 다중 클래스 분류기로의 확장에 대해서도 다루도록 하겠습니다. 

- 또한 시각화의 이유로 sepal lenght 와 petal length 두 개의 변수만 활용하고자 합니다.


```python
import matplotlib.pyplot as plt

# setosa & versicolor
y = df.iloc[0:100, 4]

y = np.where(y == 'setosa', -1, 1)
X = df.iloc[0:100, [0, 2]].values
```


```python
plt.scatter(x[:50, 0], x[:50,1], color = 'red', marker = 'o', label = 'setosa')
plt.scatter(x[50:100, 0], x[50:100, 1], color = 'blue', marker = 'x', label = 'versicolor')
plt.xlabel('sepal length')
plt.ylabel('petal length')
plt.legend(loc = 'upper left')
plt.show
```




    <function matplotlib.pyplot.show(close=None, block=None)>




​    
![png](output_8_1.png)
​    


에포크 대비 오분류 오차를 그래프로 확인해보겠습니다. 알고리즘이 수렴하여 iris class를 구분하는 결정 경계를 찾는 과정입니다.


```python
ppn = Perceptron(eta = 0.1, n_iter = 10)
ppn.fit(X,y)
plt.plot(range(1, len(ppn.errors_)+1), ppn.errors_, marker = 'o')
plt.xlabel('Epochs')
plt.ylabel('Number of updates')

plt.show
```




    <function matplotlib.pyplot.show(close=None, block=None)>




​    
![png](output_10_1.png)
​    


퍼셉트론 학습을 통해 두 개의 클래스로 구성된 iris 데이터의 샘플을 분류하는 것을 확인했습니다.


```python
from matplotlib.colors import ListedColormap

def plot_decision_regions(X, y, classifier, resolution=0.02):

    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], 
                    y=X[y == cl, 1],
                    alpha=0.8, 
                    c=colors[idx],
                    marker=markers[idx], 
                    label=cl, 
                    edgecolor='black')

```


```python
plot_decision_regions(X, y, classifier=ppn)
plt.xlabel('sepal length [cm]')
plt.ylabel('petal length [cm]')
plt.legend(loc='upper left')

# plt.savefig('images/02_08.png', dpi=300)
plt.show()
```


​    
![png](output_13_0.png)
​    


여기까지가 로젠블라트의 퍼셉트론 알고리즘이었습니다. 이후 로젠블라트 퍼셉트론 알고리즘을 개선한 아달린 퍼셉트론 알고리즘에 대해 리뷰를 하도록 하겠습니다.
