---
title: "DT(2)"
tags:
use_math: true
---

# 의사결정나무

의사결정나무(Decision tree)는 모델의 높은 해석력, 설명력이 요구되는 테스크에 적합한 방법입니다. 회귀분석과 함께 높은 설명력을 가지고 있습니다. 이번 포스트에서는 의사결정나무가 데이터를 어떻게 설명하는지 어떻게 모델이 학습되는지 살펴보도록 하겠습니다. 의사결정나무는 분류모델과 예측모델 모두 사용 가능한데 이번 리뷰에서는 분류 모델을 먼저 다뤄보고 이후 회귀모델까지 구현해볼까 합니다.

의사결정나무는 root라고 하는 시작점에서 binary 한 형태로 데이터를 분리하는데 데이터를 분리하는 기준은 정보획득량(information Gain)이 최대로 되는 지점입니다. 분리된 데이터로부터 노드가 순수해질 때까지 이러한 과정을 반복합니다. binary 하게 분리하는 이유는 간단하고 탐색공간을 줄이기 위해서 입니다. 상위 노드를 부모 노드라고 부르며 분리가 일어난 하위 노드를 자식 노드라고 합니다. 더 이상 분리될 데이터가 없을 때 까지 반복할 수 있는데, 각 데이터당 하나의 노드를 갖게할 수도 있습니다. 이러한 경우 나무의 깊이가 깊어지며, 일반적으로 과대적합 되었다고 판단합니다. 과대적합은 가지치기(pruning)을 통해 나무의 깊이를 제한하는 방법으로 해결할 수 있습니다. 


## 정보획득량

의사결정나무에서 노드를 분리하기 위해 정보획득량을 최대로 하도록 목적함수를 정의합니다. 목적함수는 아래와 같이 정의할 수 있습니다.

$$ IG(D_p,f) = I(D_p) - \sum_{j=1}^m \frac{N_j} {N_p} I(D_j) $$

여기서 $f$는 분할에 사용할 특성입니다. $D_p$ 와 $D_j$는 부모와 $j$번째 자식 노드의 데이터셋 입니다.  $I$는 불순도(impurity) 지표입니다. $N_p$는 부모 노드에 있는 전체 샘플 개수 입니다. $N_j$는 j번째 자식 노드에 있는 샘플 개수입니다. 위 식에서 볼 수 있듯이 정보 확득량은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이입니다. 자식 노드의 불순도가 낮을수록 정보 획득량은 증가하며 부모 노드에서 두 개의 자식 노드 $D_{left}$, $D_{right}$로 나누어 집니다.

$$ IG(D_p,f) = I(D_p) - \sum_{j=1}^m \frac{N_{left}} {N_p} I(D_{left}) - \frac{N_{right}} {N_p} I(D_{right}) $$

의사결정나무에서 노드를 분리하기 위해 불순도를 계산하는 지표는 세가지가 있습니다. 

### 1. 엔트로피
$$ I_H(t) = - \sum_{i=1}^c{p(i|t) log_2p(i|t)} $$


### 2. 지니 불순도
$$ I_G(t) = - \sum_{i=1}^c{p(i|t)(1-p(i|t))} = 1 - \sum_{i=1}^c{p(i|t)^2}  $$


### 3. 분류 오차
$$ I_E(t) = 1 - max{p(i|t)} $$




\.


$p(i|t)$는 특정 노트 t에서 클래스 i에 속한 샘플의 비율입니다. 한 노드에 모든 샘플이 같은 클래스라면 엔트로피는 0이 되며, 균등하게 분포되어 있으면 1이 됩니다. 즉 $p(i = 1|t) = 1$ 또는 $p(i = 0|t) = 0$ 과 같은 경우 엔트로피 값은 0이며, $p(i = 1|t) = 0.5$, $p(i = 0|t) = 0.5$ 와 같이 균등하게 분포되어 있을때 엔트로피 값은 1이 됩니다. 즉, 엔트로피는 노드의 정보 의존을 최대화하는 것으로 이해하면 될 것 같습니다. 지니 불순도는 잘못 분류될 확률을 최소화하기 위한 기준인데, 엔트로피와 마찬가지로 클래스가 균등하게 분포되어 있을 때 최대가 됩니다. 실제로 그 결과값은 매우 비슷하기 때문에 의사결정나무를 모델링하고, 튜닝할 때 정보획득지표를 바꾸기 보다는 가지치기 수준을 고려하는 것이 훨씬 효율적이라고 할 수 있겠습니다.

아래 A, B의 경우에 대해서 엔트로피를 계산해보겠습니다.  
Class1, Class2 각각 40개씩 총 80개의 데이터가 있습니다.  
A 의 경우에는 왼쪽 노드에 Class1 30개, Class2 10개. 오른쪽 노드에 Class1 10개, Class2 30개  
B 의 경우에는 왼쪽 노드에 Class1 20개, Class2 40개. 오른쪽 노드에 Class1 20개, Class2 0개 로 나뉜 상황입니다. 

두 의사결정나무에서 엔트로피 값의 차이를 확인해 보겠습니다.

![sinario](C:/Users/94tjd/Downloads/DT/sinario.png)

$$ I_H(D_p) = -\,(0.5\,log_2\,(0.5) + 0.5\,log_2\,(0.5)) = 1 $$  
$$ A:I_H(D_{left}) = - \left( \frac{3}{4}log_2\left(\frac{3}{4}\right)+\frac{1}{4}log_2\left(\frac{1}{4}\right)\right) = 0.81 $$  


$$ A:I_H(D_{right}) = - \left( \frac{1}{4}log_2\left(\frac{1}{4}\right)+\frac{3}{4}log_2\left(\frac{3}{4}\right)\right) = 0.81 $$  


$$ \therefore A : IG_H = 1 - \frac{4}{8}0.81 -\frac{4}{8}0.81 = 0.19 $$

.
<center> A의 왼쪽 노드의 불순도는 0.81, 오른쪽 노드의 불순도 0.81로, A 트리의 정보획득량은 0.19입니다. </center>

$$ B:I_H(D_{left}) = - \left( \frac{2}{6}log_2\left(\frac{2}{6}\right)+\frac{4}{6}log_2\left(\frac{4}{6}\right)\right) = 0.92 $$  

$$ B:I_H(D_{right}) = 0$$ 

$$ \therefore B : IG_H = 1 - \frac{6}{8}0.92 = 0.31 $$
.
<center> B의 왼쪽 노드의 불순도는 0.92, 오른쪽 노드의 불순도 0으로, B 트리의 정보획득량은 0.31입니다. </center>

따라서 A,B 트리 중 'B 트리가 더 많은 정보를 가지고 있다' 라고 생각하면 될 것 같습니다.  
위 경우는 이미 만들어진 두 개의 의사결정나무에 대한 정보획득량 계산일 뿐 가장 적절한 의사결정나무는 아닙니다. 실제 의사결정나무를 모델링할 때는 위에서 언급했듯이 가장 많은 정보를 갖도록 노드가 나뉘게 됩니다. 

지니 불순도, 분류 오차의 계산도 엔트로피 계산과 유사한데요, 시각적으로 비교해보겠습니다.


```python
import matplotlib.pyplot as plt
import numpy as np

def gini(p):
    return p * (1 - p) + (1 - p) * (1 - (1 - p))

def entropy(p):
    return - p * np.log2(p) - (1 - p) * np.log2((1 - p))

def error(p):
    return 1 - np.max([p, 1 - p])

x = np.arange(0.0, 1.0, 0.01)

ent = [entropy(p) if p != 0 else None for p in x]
sc_ent = [e * 0.5 if e else None for e in ent]
err = [error(i) for i in x]

fig = plt.figure()
ax = plt.subplot(111)

for i, lab, ls, c, in zip([ent, sc_ent, gini(x), err], 
                          ['Entropy', 'Entropy (scaled)', 
                           'Gini impurity', 'Misclassification error'],
                          ['-', '-', '--', '-.'],
                          ['black', 'lightgray', 'red', 'green', 'cyan']):
    line = ax.plot(x, i, label=lab, linestyle=ls, lw=2, color=c)

ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),
          ncol=5, fancybox=True, shadow=False)

ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')
ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')
plt.ylim([0, 1.1])
plt.xlabel('p(i=1)')
plt.ylabel('impurity index')
plt.show()
```


![png](C:/Users/94tjd/Downloads/DT/output_5_0.png)
    


## 의사결정나무 생성

의사결정나무의 목적함수 정보획득량과 이를 구성하는 불순도에 대해서 확인했습니다. 
이번엔 iris 예제 데이터를 통해 의사결정나무를 만들어보도록 하겠습니다.
sklearn에서 제공하는 DecisionTreeClassifier를 사용했으며, 간단한 구현을 목적으로 했기 때문에 max_depth를 4로 설정하겠습니다. depth 지정은 노드 분기를 최대 4번까지로 제한하는 hyper-parameter입니다.  불순도는 entropy를 사용하였습니다.


```python
from sklearn import datasets
import numpy as np
iris = datasets.load_iris()
x = iris.data[:, [2,3]]
y = iris.target
print('클래스 :', np.unique(y))

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y,
                                                    test_size = 0.3,
                                                    random_state = 1,
                                                    stratify = y)

from sklearn.tree import DecisionTreeClassifier
from mlxtend.plotting import plot_decision_regions
tree_model = DecisionTreeClassifier(criterion='entropy',
                                    max_depth=4,
                                    random_state=1)
tree_model.fit(X_train, y_train)
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, clf = tree_model)

plt.xlabel('petal length [cm]')
plt.ylabel('petal width [cm]')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()
```

    클래스 : [0 1 2]




![png](C:/Users/94tjd/Downloads/DT/output_7_1.png)
    



```python
### 플랏1
from sklearn import tree
tree.plot_tree(tree_model)
plt.show()
```


![png](C:/Users/94tjd/Downloads/DT/output_8_0.png)
    



```python
### 플랏2
tree.plot_tree(tree_model,
               impurity=True,
               filled=True,
               rounded=True,
               class_names=['Setosa',
                            'Versicolor',
                            'Virginica'],
               feature_names=['petal length',
                              'petal width'])
plt.show()
```

    findfont: Font family ['NanumGothic'] not found. Falling back to DejaVu Sans.




![png](C:/Users/94tjd/Downloads/DT/output_9_1.png)
    

